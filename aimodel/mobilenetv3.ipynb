{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4718786,"sourceType":"datasetVersion","datasetId":2730182}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-27T11:13:53.827807Z","iopub.execute_input":"2025-02-27T11:13:53.828300Z","iopub.status.idle":"2025-02-27T11:13:57.572679Z","shell.execute_reply.started":"2025-02-27T11:13:53.828276Z","shell.execute_reply":"2025-02-27T11:13:57.571837Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#1\nimport os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.layers import LSTM, Dense, TimeDistributed, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Sequential\nimport shutil\nfrom sklearn.model_selection import train_test_split\n\n# Define input and output paths\nINPUT_PATH = '/kaggle/input/rwf2000/RWF-2000'\nOUTPUT_PATH = '/kaggle/working'\n\ndef combine_datasets():\n    # Create new directories for combined dataset\n    combined_path = os.path.join(OUTPUT_PATH, 'combined_dataset')\n    os.makedirs(os.path.join(combined_path, 'Fight'), exist_ok=True)\n    os.makedirs(os.path.join(combined_path, 'NonFight'), exist_ok=True)\n    \n    # Define source directories\n    train_fight = os.path.join(INPUT_PATH, 'train', 'Fight')\n    val_fight = os.path.join(INPUT_PATH, 'val', 'Fight')\n    train_nonfight = os.path.join(INPUT_PATH, 'train', 'NonFight')\n    val_nonfight = os.path.join(INPUT_PATH, 'val', 'NonFight')\n    \n    # Copy all videos to combined directories\n    for src_dir, dest_dir in [(train_fight, 'Fight'), (val_fight, 'Fight'),\n                             (train_nonfight, 'NonFight'), (val_nonfight, 'NonFight')]:\n        for video in os.listdir(src_dir):\n            shutil.copy2(\n                os.path.join(src_dir, video),\n                os.path.join(combined_path, dest_dir, video)\n            )\n    \n    print(f\"Combined dataset created at: {combined_path}\")\n    return combined_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T10:46:23.997320Z","iopub.execute_input":"2025-02-27T10:46:23.997822Z","iopub.status.idle":"2025-02-27T10:46:36.756817Z","shell.execute_reply.started":"2025-02-27T10:46:23.997794Z","shell.execute_reply":"2025-02-27T10:46:36.755893Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#2\ndef extract_frames(video_path, num_frames=30):\n    frames = []\n    cap = cv2.VideoCapture(video_path)\n    \n    if not cap.isOpened():\n        print(f\"Error opening video file: {video_path}\")\n        return None\n    \n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    indices = np.linspace(0, total_frames-1, num_frames, dtype=int)\n    \n    for idx in indices:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n        ret, frame = cap.read()\n        if ret:\n            frame = cv2.resize(frame, (224, 224))\n            frame = frame / 255.0\n            frames.append(frame)\n    \n    cap.release()\n    \n    if len(frames) != num_frames:\n        print(f\"Warning: Could not extract {num_frames} frames from {video_path}\")\n        return None\n        \n    return np.array(frames)\n\ndef prepare_dataset(combined_path, num_frames=30):\n    X = []\n    y = []\n    \n    # Process Fight videos\n    fight_path = os.path.join(combined_path, 'Fight')\n    print(\"Processing Fight videos...\")\n    for i, video in enumerate(os.listdir(fight_path)):\n        frames = extract_frames(os.path.join(fight_path, video), num_frames)\n        if frames is not None:\n            X.append(frames)\n            y.append(1)\n        if (i + 1) % 100 == 0:\n            print(f\"Processed {i + 1} Fight videos\")\n    \n    # Process NonFight videos\n    nonfight_path = os.path.join(combined_path, 'NonFight')\n    print(\"\\nProcessing NonFight videos...\")\n    for i, video in enumerate(os.listdir(nonfight_path)):\n        frames = extract_frames(os.path.join(nonfight_path, video), num_frames)\n        if frames is not None:\n            X.append(frames)\n            y.append(0)\n        if (i + 1) % 100 == 0:\n            print(f\"Processed {i + 1} NonFight videos\")\n    \n    return np.array(X), np.array(y)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T10:46:36.758301Z","iopub.execute_input":"2025-02-27T10:46:36.758785Z","iopub.status.idle":"2025-02-27T10:46:36.767584Z","shell.execute_reply.started":"2025-02-27T10:46:36.758761Z","shell.execute_reply":"2025-02-27T10:46:36.766714Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#3\ndef create_model(num_frames=30):\n    base_model = MobileNetV2(\n        input_shape=(224, 224, 3),\n        include_top=False,\n        weights='imagenet'\n    )\n    \n    base_model.trainable = False\n    \n    model = Sequential([\n        TimeDistributed(base_model, input_shape=(num_frames, 224, 224, 3)),\n        TimeDistributed(GlobalAveragePooling2D()),\n        LSTM(256, return_sequences=True),\n        LSTM(128),\n        Dense(64, activation='relu'),\n        Dense(1, activation='sigmoid')\n    ])\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T10:46:36.768626Z","iopub.execute_input":"2025-02-27T10:46:36.768854Z","iopub.status.idle":"2025-02-27T10:46:36.797278Z","shell.execute_reply.started":"2025-02-27T10:46:36.768825Z","shell.execute_reply":"2025-02-27T10:46:36.796630Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#222222\ndef train_model():\n    # Set parameters\n    num_frames = 30\n    batch_size = 8\n    epochs = 50  # Increased to 50 epochs\n    \n    # Combine datasets\n    print(\"Combining datasets...\")\n    combined_path = combine_datasets()\n    \n    # Prepare dataset\n    print(\"\\nPreparing dataset...\")\n    X, y = prepare_dataset(combined_path, num_frames)\n    \n    # Save preprocessed data\n    np.save(os.path.join(OUTPUT_PATH, 'X_preprocessed.npy'), X)\n    np.save(os.path.join(OUTPUT_PATH, 'y_preprocessed.npy'), y)\n    print(f\"\\nPreprocessed data saved to {OUTPUT_PATH}\")\n    \n    # First split: separate test set (20% of total data)\n    X_temp, X_test, y_temp, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=y\n    )\n    \n    # Second split: divide remaining data into train (87.5%) and validation (12.5%)\n    # This gives us 70% train and 10% validation of the total data\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_temp, y_temp, test_size=0.125, random_state=42, stratify=y_temp\n    )\n    \n    print(f\"\\nData split sizes:\")\n    print(f\"Training samples: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n    print(f\"Validation samples: {len(X_val)} ({len(X_val)/len(X)*100:.1f}%)\")\n    print(f\"Test samples: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n    \n    # Create and compile model\n    print(\"\\nCreating and compiling model...\")\n    model = create_model(num_frames)\n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    # Create checkpoint callback\n    checkpoint_path = os.path.join(OUTPUT_PATH, 'best_model.h5')\n    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n        checkpoint_path,\n        monitor='val_accuracy',\n        save_best_only=True,\n        mode='max'\n    )\n    \n    # Add learning rate reduction callback\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.2,\n        patience=3,\n        min_lr=1e-6\n    )\n    \n    # Train model\n    print(\"\\nTraining model...\")\n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_val, y_val),\n        batch_size=batch_size,\n        epochs=epochs,\n        callbacks=[\n            checkpoint_callback,\n            reduce_lr,\n            tf.keras.callbacks.EarlyStopping(\n                monitor='val_loss',\n                patience=7,\n                restore_best_weights=True\n            )\n        ]\n    )\n    \n    # Save training history\n    np.save(os.path.join(OUTPUT_PATH, 'training_history.npy'), history.history)\n    \n    # Evaluate model\n    print(\"\\nEvaluating model...\")\n    evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test)\n    \n    return model, history","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#2222222\ndef evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test):\n    # Evaluate on training set\n    train_loss, train_accuracy = model.evaluate(X_train, y_train)\n    print(f\"\\nTraining accuracy: {train_accuracy*100:.2f}%\")\n    \n    # Evaluate on validation set\n    val_loss, val_accuracy = model.evaluate(X_val, y_val)\n    print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n    \n    # Evaluate on test set\n    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n    print(f\"Test accuracy: {test_accuracy*100:.2f}%\")\n    \n    # Make predictions on test set\n    predictions = model.predict(X_test)\n    predictions = (predictions > 0.5).astype(int)\n    \n    # Calculate metrics\n    from sklearn.metrics import classification_report, confusion_matrix\n    \n    # Save and print classification report\n    report = classification_report(y_test, predictions)\n    with open(os.path.join(OUTPUT_PATH, 'classification_report.txt'), 'w') as f:\n        f.write(\"Test Set Classification Report:\\n\")\n        f.write(report)\n        f.write(f\"\\nTraining Accuracy: {train_accuracy*100:.2f}%\\n\")\n        f.write(f\"Validation Accuracy: {val_accuracy*100:.2f}%\\n\")\n        f.write(f\"Test Accuracy: {test_accuracy*100:.2f}%\\n\")\n    \n    print(\"\\nClassification Report:\")\n    print(report)\n    \n    # Save and print confusion matrix\n    cm = confusion_matrix(y_test, predictions)\n    np.save(os.path.join(OUTPUT_PATH, 'confusion_matrix.npy'), cm)\n    print(\"\\nConfusion Matrix:\")\n    print(cm)\n    \n    # Plot training history\n    import matplotlib.pyplot as plt\n    \n    plt.figure(figsize=(12, 4))\n    \n    # Plot accuracy\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'], label='Training Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.title('Model Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    # Plot loss\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='Training Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title('Model Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_PATH, 'training_history.png'))\n    plt.close()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#4\ndef train_model():\n    # Set parameters\n    num_frames = 30\n    batch_size = 8\n    epochs = 20\n    \n    # Combine datasets\n    print(\"Combining datasets...\")\n    combined_path = combine_datasets()\n    \n    # Prepare dataset\n    print(\"\\nPreparing dataset...\")\n    X, y = prepare_dataset(combined_path, num_frames)\n    \n    # Save preprocessed data\n    np.save(os.path.join(OUTPUT_PATH, 'X_preprocessed.npy'), X)\n    np.save(os.path.join(OUTPUT_PATH, 'y_preprocessed.npy'), y)\n    print(f\"\\nPreprocessed data saved to {OUTPUT_PATH}\")\n    \n    # Split dataset\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=y\n    )\n    \n    # Create and compile model\n    print(\"\\nCreating and compiling model...\")\n    model = create_model(num_frames)\n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    # Create checkpoint callback\n    checkpoint_path = os.path.join(OUTPUT_PATH, 'best_model.h5')\n    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n        checkpoint_path,\n        monitor='val_accuracy',\n        save_best_only=True,\n        mode='max'\n    )\n    \n    # Train model\n    print(\"\\nTraining model...\")\n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_test, y_test),\n        batch_size=batch_size,\n        epochs=epochs,\n        callbacks=[\n            checkpoint_callback,\n            tf.keras.callbacks.EarlyStopping(\n                monitor='val_loss',\n                patience=5,\n                restore_best_weights=True\n            )\n        ]\n    )\n    \n    # Save training history\n    np.save(os.path.join(OUTPUT_PATH, 'training_history.npy'), history.history)\n    \n    # Evaluate model\n    print(\"\\nEvaluating model...\")\n    evaluate_model(model, X_test, y_test)\n    \n    return model, history\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T10:46:36.797985Z","iopub.execute_input":"2025-02-27T10:46:36.798198Z","iopub.status.idle":"2025-02-27T10:46:36.823111Z","shell.execute_reply.started":"2025-02-27T10:46:36.798179Z","shell.execute_reply":"2025-02-27T10:46:36.822270Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#5\ndef evaluate_model(model, X_test, y_test):\n    # Evaluate model\n    loss, accuracy = model.evaluate(X_test, y_test)\n    print(f\"Test accuracy: {accuracy*100:.2f}%\")\n    \n    # Make predictions\n    predictions = model.predict(X_test)\n    predictions = (predictions > 0.5).astype(int)\n    \n    # Calculate metrics\n    from sklearn.metrics import classification_report, confusion_matrix\n    \n    # Save and print classification report\n    report = classification_report(y_test, predictions)\n    with open(os.path.join(OUTPUT_PATH, 'classification_report.txt'), 'w') as f:\n        f.write(report)\n    print(\"\\nClassification Report:\")\n    print(report)\n    \n    # Save and print confusion matrix\n    cm = confusion_matrix(y_test, predictions)\n    np.save(os.path.join(OUTPUT_PATH, 'confusion_matrix.npy'), cm)\n    print(\"\\nConfusion Matrix:\")\n    print(cm)    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T10:46:36.823824Z","iopub.execute_input":"2025-02-27T10:46:36.824139Z","iopub.status.idle":"2025-02-27T10:46:36.846374Z","shell.execute_reply.started":"2025-02-27T10:46:36.824111Z","shell.execute_reply":"2025-02-27T10:46:36.845760Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#6\nif __name__ == \"__main__\":\n    print(\"Starting violence detection model training...\")\n    model, history = train_model()\n    print(\"\\nTraining completed. All results saved to:\", OUTPUT_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T10:46:36.847109Z","iopub.execute_input":"2025-02-27T10:46:36.847292Z","execution_failed":"2025-02-27T11:00:02.874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.layers import LSTM, Dense, TimeDistributed, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Sequential\nimport shutil\nfrom sklearn.model_selection import train_test_split\n\n# Define input and output paths\nINPUT_PATH = '/kaggle/input/rwf2000/RWF-2000'\nOUTPUT_PATH = '/kaggle/working'\n\ndef combine_datasets():\n    # Create new directories for combined dataset\n    combined_path = os.path.join(OUTPUT_PATH, 'combined_dataset')\n    os.makedirs(os.path.join(combined_path, 'Fight'), exist_ok=True)\n    os.makedirs(os.path.join(combined_path, 'NonFight'), exist_ok=True)\n    \n    # Define source directories\n    train_fight = os.path.join(INPUT_PATH, 'train', 'Fight')\n    val_fight = os.path.join(INPUT_PATH, 'val', 'Fight')\n    train_nonfight = os.path.join(INPUT_PATH, 'train', 'NonFight')\n    val_nonfight = os.path.join(INPUT_PATH, 'val', 'NonFight')\n    \n    # Copy all videos to combined directories\n    for src_dir, dest_dir in [(train_fight, 'Fight'), (val_fight, 'Fight'),\n                             (train_nonfight, 'NonFight'), (val_nonfight, 'NonFight')]:\n        for video in os.listdir(src_dir):\n            shutil.copy2(\n                os.path.join(src_dir, video),\n                os.path.join(combined_path, dest_dir, video)\n            )\n    \n    print(f\"Combined dataset created at: {combined_path}\")\n    return combined_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T11:14:17.531215Z","iopub.execute_input":"2025-02-27T11:14:17.531543Z","iopub.status.idle":"2025-02-27T11:14:30.004028Z","shell.execute_reply.started":"2025-02-27T11:14:17.531520Z","shell.execute_reply":"2025-02-27T11:14:30.003148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_frames(video_path, num_frames=20):  # Reduced from 30 to 20 frames\n    frames = []\n    cap = cv2.VideoCapture(video_path)\n    \n    if not cap.isOpened():\n        print(f\"Error opening video file: {video_path}\")\n        return None\n    \n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    indices = np.linspace(0, total_frames-1, num_frames, dtype=int)\n    \n    for idx in indices:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n        ret, frame = cap.read()\n        if ret:\n            # Reduce image size from 224x224 to 160x160\n            frame = cv2.resize(frame, (160, 160))\n            # Convert to float16 instead of float32\n            frame = (frame / 255.0).astype(np.float16)\n            frames.append(frame)\n    \n    cap.release()\n    return np.array(frames) if frames else None\n\ndef prepare_dataset(combined_path, num_frames=20, batch_size=32):\n    X = []\n    y = []\n    count = 0\n    \n    # Process Fight videos\n    fight_path = os.path.join(combined_path, 'Fight')\n    print(\"Processing Fight videos...\")\n    \n    # Process videos in batches and save immediately\n    for i, video in enumerate(os.listdir(fight_path)):\n        frames = extract_frames(os.path.join(fight_path, video), num_frames)\n        if frames is not None:\n            X.append(frames)\n            y.append(1)\n            count += 1\n            \n        # Save batch and clear memory\n        if count % batch_size == 0:\n            X_batch = np.array(X)\n            y_batch = np.array(y)\n            \n            # Save batch\n            batch_number = count // batch_size\n            np.save(os.path.join(OUTPUT_PATH, f'X_batch_{batch_number}.npy'), X_batch)\n            np.save(os.path.join(OUTPUT_PATH, f'y_batch_{batch_number}.npy'), y_batch)\n            \n            # Clear memory\n            X = []\n            y = []\n            \n        if (i + 1) % 100 == 0:\n            print(f\"Processed {i + 1} Fight videos\")\n    \n    # Same process for NonFight videos\n    nonfight_path = os.path.join(combined_path, 'NonFight')\n    print(\"\\nProcessing NonFight videos...\")\n    \n    for i, video in enumerate(os.listdir(nonfight_path)):\n        frames = extract_frames(os.path.join(nonfight_path, video), num_frames)\n        if frames is not None:\n            X.append(frames)\n            y.append(0)\n            count += 1\n            \n        if count % batch_size == 0:\n            X_batch = np.array(X)\n            y_batch = np.array(y)\n            \n            batch_number = count // batch_size\n            np.save(os.path.join(OUTPUT_PATH, f'X_batch_{batch_number}.npy'), X_batch)\n            np.save(os.path.join(OUTPUT_PATH, f'y_batch_{batch_number}.npy'), y_batch)\n            \n            X = []\n            y = []\n            \n        if (i + 1) % 100 == 0:\n            print(f\"Processed {i + 1} NonFight videos\")\n    \n    # Save any remaining samples\n    if X:\n        X_batch = np.array(X)\n        y_batch = np.array(y)\n        batch_number = (count // batch_size) + 1\n        np.save(os.path.join(OUTPUT_PATH, f'X_batch_{batch_number}.npy'), X_batch)\n        np.save(os.path.join(OUTPUT_PATH, f'y_batch_{batch_number}.npy'), y_batch)\n    \n    return count, batch_number\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T11:14:31.630949Z","iopub.execute_input":"2025-02-27T11:14:31.631498Z","iopub.status.idle":"2025-02-27T11:14:31.642285Z","shell.execute_reply.started":"2025-02-27T11:14:31.631470Z","shell.execute_reply":"2025-02-27T11:14:31.641417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, batch_numbers, batch_size, output_path):\n        self.batch_numbers = batch_numbers\n        self.batch_size = batch_size\n        self.output_path = output_path\n        \n    def __len__(self):\n        return len(self.batch_numbers)\n    \n    def __getitem__(self, idx):\n        batch_number = self.batch_numbers[idx]\n        X = np.load(os.path.join(self.output_path, f'X_batch_{batch_number}.npy'))\n        y = np.load(os.path.join(self.output_path, f'y_batch_{batch_number}.npy'))\n        return X, y\n\ndef create_model(num_frames=20):\n    base_model = MobileNetV2(\n        input_shape=(160, 160, 3),  # Reduced input size\n        include_top=False,\n        weights='imagenet'\n    )\n    \n    base_model.trainable = False\n    \n    model = Sequential([\n        TimeDistributed(base_model, input_shape=(num_frames, 160, 160, 3)),\n        TimeDistributed(GlobalAveragePooling2D()),\n        LSTM(128, return_sequences=True),  # Reduced from 256 to 128\n        LSTM(64),  # Reduced from 128 to 64\n        Dense(32, activation='relu'),  # Reduced from 64 to 32\n        Dense(1, activation='sigmoid')\n    ])\n    \n    return model\n    \ndef train_model():\n    num_frames = 20\n    batch_size = 32\n    epochs = 50\n    \n    # First combine the datasets\n    print(\"Combining datasets...\")\n    combined_path = combine_datasets()\n    \n    # Prepare dataset and save in batches\n    print(\"Preparing dataset...\")\n    total_samples, num_batches = prepare_dataset(combined_path, num_frames, batch_size)\n    \n    # Create batch numbers for train/val/test split\n    batch_numbers = list(range(1, num_batches + 1))\n    np.random.shuffle(batch_numbers)\n    \n    # Split batch numbers\n    train_idx = int(0.7 * len(batch_numbers))\n    val_idx = int(0.8 * len(batch_numbers))\n    \n    train_batches = batch_numbers[:train_idx]\n    val_batches = batch_numbers[train_idx:val_idx]\n    test_batches = batch_numbers[val_idx:]\n    \n    # Create data generators\n    train_generator = DataGenerator(train_batches, batch_size, OUTPUT_PATH)\n    val_generator = DataGenerator(val_batches, batch_size, OUTPUT_PATH)\n    test_generator = DataGenerator(test_batches, batch_size, OUTPUT_PATH)\n    \n    # Create and train model\n    print(\"\\nCreating and compiling model...\")\n    model = create_model(num_frames)\n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    # Create callbacks\n    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n        filepath=os.path.join(OUTPUT_PATH, 'best_model.keras'),  # Changed from .h5 to .keras\n        monitor='val_accuracy',\n        save_best_only=True,\n        mode='max'\n    )\n    \n    early_stopping = tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=5,\n        restore_best_weights=True\n    )\n    \n    # Add learning rate reduction callback\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.2,\n        patience=3,\n        min_lr=1e-6\n    )\n    \n    print(\"\\nStarting model training...\")\n    history = model.fit(\n        train_generator,\n        validation_data=val_generator,\n        epochs=epochs,\n        callbacks=[\n            checkpoint_callback,\n            early_stopping,\n            reduce_lr\n        ]\n    )\n    \n    # Evaluate model\n    print(\"\\nEvaluating model...\")\n    test_loss, test_accuracy = model.evaluate(test_generator)\n    print(f\"\\nTest accuracy: {test_accuracy*100:.2f}%\")\n    \n    # Save training history\n    np.save(os.path.join(OUTPUT_PATH, 'training_history.npy'), history.history)\n    \n    # Save final model\n    model.save(os.path.join(OUTPUT_PATH, 'final_model.keras'))\n    \n    return model, history\n\ndef model2():\n    num_frames = 20\n    batch_size = 32\n    epochs = 50\n    num_batches=20\n    # Create batch numbers for train/val/test split\n    batch_numbers = list(range(1, num_batches + 1))\n    np.random.shuffle(batch_numbers)\n    \n    # Split batch numbers\n    train_idx = int(0.7 * len(batch_numbers))\n    val_idx = int(0.8 * len(batch_numbers))\n    \n    train_batches = batch_numbers[:train_idx]\n    val_batches = batch_numbers[train_idx:val_idx]\n    test_batches = batch_numbers[val_idx:]\n    train_generator = DataGenerator(train_batches, batch_size, OUTPUT_PATH)\n    val_generator = DataGenerator(val_batches, batch_size, OUTPUT_PATH)\n    test_generator = DataGenerator(test_batches, batch_size, OUTPUT_PATH)\n    \n    # Create and train model\n    print(\"\\nCreating and compiling model...\")\n    model = create_model(num_frames)\n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    # Create callbacks\n    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n        filepath=os.path.join(OUTPUT_PATH, 'best_model.keras'),  # Changed from .h5 to .keras\n        monitor='val_accuracy',\n        save_best_only=True,\n        mode='max'\n    )\n    \n    early_stopping = tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=5,\n        restore_best_weights=True\n    )\n    \n    # Add learning rate reduction callback\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.2,\n        patience=3,\n        min_lr=1e-6\n    )\n    \n    print(\"\\nStarting model training...\")\n    history = model.fit(\n        train_generator,\n        validation_data=val_generator,\n        epochs=epochs,\n        callbacks=[\n            checkpoint_callback,\n            early_stopping,\n            reduce_lr\n        ]\n    )\n    \n    # Evaluate model\n    print(\"\\nEvaluating model...\")\n    test_loss, test_accuracy = model.evaluate(test_generator)\n    print(f\"\\nTest accuracy: {test_accuracy*100:.2f}%\")\n    \n    # Save training history\n    np.save(os.path.join(OUTPUT_PATH, 'training_history.npy'), history.history)\n    \n    # Save final model\n    model.save(os.path.join(OUTPUT_PATH, 'final_model.keras'))\n    \n    return model, history\n\ndef evaluate_model(model, X_test, y_test):\n    # Evaluate model\n    loss, accuracy = model.evaluate(X_test, y_test)\n    print(f\"Test accuracy: {accuracy*100:.2f}%\")\n    \n    # Make predictions\n    predictions = model.predict(X_test)\n    predictions = (predictions > 0.5).astype(int)\n    \n    # Calculate metrics\n    from sklearn.metrics import classification_report, confusion_matrix\n    \n    # Save and print classification report\n    report = classification_report(y_test, predictions)\n    with open(os.path.join(OUTPUT_PATH, 'classification_report.txt'), 'w') as f:\n        f.write(report)\n    print(\"\\nClassification Report:\")\n    print(report)\n    \n    # Save and print confusion matrix\n    cm = confusion_matrix(y_test, predictions)\n    np.save(os.path.join(OUTPUT_PATH, 'confusion_matrix.npy'), cm)\n    print(\"\\nConfusion Matrix:\")\n    print(cm)    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T11:41:44.050737Z","iopub.execute_input":"2025-02-27T11:41:44.051071Z","iopub.status.idle":"2025-02-27T11:41:44.066509Z","shell.execute_reply.started":"2025-02-27T11:41:44.051049Z","shell.execute_reply":"2025-02-27T11:41:44.065674Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    print(\"Starting violence detection model training...\")\n    model, history = train_model()\n    print(\"\\nTraining completed. All results saved to:\", OUTPUT_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T11:16:55.277836Z","iopub.execute_input":"2025-02-27T11:16:55.278150Z","iopub.status.idle":"2025-02-27T11:33:01.697157Z","shell.execute_reply.started":"2025-02-27T11:16:55.278127Z","shell.execute_reply":"2025-02-27T11:33:01.693342Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model2()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T11:41:50.399743Z","iopub.execute_input":"2025-02-27T11:41:50.400055Z","iopub.status.idle":"2025-02-27T11:51:53.862026Z","shell.execute_reply.started":"2025-02-27T11:41:50.400031Z","shell.execute_reply":"2025-02-27T11:51:53.861138Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.layers import LSTM, Dense, TimeDistributed, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Sequential\nimport shutil\nfrom sklearn.model_selection import train_test_split\n\n# Define paths\nINPUT_PATH = '/kaggle/input/rwf2000/RWF-2000'\nOUTPUT_PATH = '/kaggle/working'\n\ndef prepare_split_datasets():\n    \"\"\"\n    Prepare train, validation, and test datasets with specific splits:\n    Train: 600 fight + 600 non-fight\n    Val: 200 fight + 200 non-fight\n    Test: 200 fight + 200 non-fight\n    \"\"\"\n    # Create directories\n    for split in ['train', 'val', 'test']:\n        for label in ['Fight', 'NonFight']:\n            os.makedirs(os.path.join(OUTPUT_PATH, split, label), exist_ok=True)\n    \n    # Process Fight videos\n    fight_videos = []\n    fight_path = os.path.join(INPUT_PATH, 'train', 'Fight')\n    fight_videos.extend([os.path.join(fight_path, f) for f in os.listdir(fight_path)])\n    fight_path = os.path.join(INPUT_PATH, 'val', 'Fight')\n    fight_videos.extend([os.path.join(fight_path, f) for f in os.listdir(fight_path)])\n    \n    # Process NonFight videos\n    nonfight_videos = []\n    nonfight_path = os.path.join(INPUT_PATH, 'train', 'NonFight')\n    nonfight_videos.extend([os.path.join(nonfight_path, f) for f in os.listdir(nonfight_path)])\n    nonfight_path = os.path.join(INPUT_PATH, 'val', 'NonFight')\n    nonfight_videos.extend([os.path.join(nonfight_path, f) for f in os.listdir(nonfight_path)])\n    \n    # Random shuffle\n    np.random.shuffle(fight_videos)\n    np.random.shuffle(nonfight_videos)\n    \n    # Split videos\n    fight_train = fight_videos[:600]\n    fight_val = fight_videos[600:800]\n    fight_test = fight_videos[800:1000]\n    \n    nonfight_train = nonfight_videos[:600]\n    nonfight_val = nonfight_videos[600:800]\n    nonfight_test = nonfight_videos[800:1000]\n    \n    # Copy videos to respective directories\n    splits = {\n        'train': (fight_train, nonfight_train),\n        'val': (fight_val, nonfight_val),\n        'test': (fight_test, nonfight_test)\n    }\n    \n    for split_name, (fight_split, nonfight_split) in splits.items():\n        print(f\"\\nProcessing {split_name} split:\")\n        \n        # Copy fight videos\n        for video_path in fight_split:\n            video_name = os.path.basename(video_path)\n            dest_path = os.path.join(OUTPUT_PATH, split_name, 'Fight', video_name)\n            shutil.copy2(video_path, dest_path)\n        \n        # Copy nonfight videos\n        for video_path in nonfight_split:\n            video_name = os.path.basename(video_path)\n            dest_path = os.path.join(OUTPUT_PATH, split_name, 'NonFight', video_name)\n            shutil.copy2(video_path, dest_path)\n        \n        print(f\"Fight videos: {len(fight_split)}\")\n        print(f\"NonFight videos: {len(nonfight_split)}\")\n    \n    return True\n\ndef extract_frames(video_path, num_frames=30):\n    frames = []\n    cap = cv2.VideoCapture(video_path)\n    \n    if not cap.isOpened():\n        print(f\"Error opening video file: {video_path}\")\n        return None\n    \n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    indices = np.linspace(0, total_frames-1, num_frames, dtype=int)\n    \n    for idx in indices:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n        ret, frame = cap.read()\n        if ret:\n            frame = cv2.resize(frame, (224, 224))  # Using full resolution\n            frame = frame / 255.0\n            frames.append(frame)\n    \n    cap.release()\n    return np.array(frames) if frames else None\n\nclass VideoDataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, base_path, batch_size=16):\n        self.base_path = base_path\n        self.batch_size = batch_size\n        \n        # Get all video paths\n        self.fight_videos = [os.path.join(base_path, 'Fight', f) \n                           for f in os.listdir(os.path.join(base_path, 'Fight'))]\n        self.nonfight_videos = [os.path.join(base_path, 'NonFight', f) \n                              for f in os.listdir(os.path.join(base_path, 'NonFight'))]\n        \n        self.video_paths = self.fight_videos + self.nonfight_videos\n        self.labels = [1] * len(self.fight_videos) + [0] * len(self.nonfight_videos)\n        \n        # Shuffle the data\n        p = np.random.permutation(len(self.video_paths))\n        self.video_paths = np.array(self.video_paths)[p]\n        self.labels = np.array(self.labels)[p]\n    \n    def __len__(self):\n        return len(self.video_paths) // self.batch_size\n    \n    def __getitem__(self, idx):\n        batch_videos = self.video_paths[idx*self.batch_size:(idx+1)*self.batch_size]\n        batch_labels = self.labels[idx*self.batch_size:(idx+1)*self.batch_size]\n        \n        X = []\n        y = []\n        \n        for video_path, label in zip(batch_videos, batch_labels):\n            frames = extract_frames(video_path)\n            if frames is not None:\n                X.append(frames)\n                y.append(label)\n        \n        return np.array(X), np.array(y)\n\ndef create_model(num_frames=30):\n    base_model = MobileNetV2(\n        input_shape=(224, 224, 3),\n        include_top=False,\n        weights='imagenet'\n    )\n    \n    base_model.trainable = False\n    \n    model = Sequential([\n        TimeDistributed(base_model, input_shape=(num_frames, 224, 224, 3)),\n        TimeDistributed(GlobalAveragePooling2D()),\n        LSTM(256, return_sequences=True),\n        LSTM(128),\n        Dense(64, activation='relu'),\n        Dense(1, activation='sigmoid')\n    ])\n    \n    return model\n\n# Main training function\ndef train_model():\n    # Prepare datasets\n    print(\"Preparing datasets...\")\n    prepare_split_datasets()\n    \n    # Create data generators\n    train_generator = VideoDataGenerator(os.path.join(OUTPUT_PATH, 'train'), batch_size=16)\n    val_generator = VideoDataGenerator(os.path.join(OUTPUT_PATH, 'val'), batch_size=16)\n    test_generator = VideoDataGenerator(os.path.join(OUTPUT_PATH, 'test'), batch_size=16)\n    \n    # Create and compile model\n    print(\"\\nCreating and compiling model...\")\n    model = create_model()\n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    # Create callbacks\n    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n        filepath=os.path.join(OUTPUT_PATH, 'best_model.keras'),\n        monitor='val_accuracy',\n        save_best_only=True,\n        mode='max'\n    )\n    \n    early_stopping = tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=7,\n        restore_best_weights=True\n    )\n    \n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.2,\n        patience=3,\n        min_lr=1e-6\n    )\n    \n    # Train model\n    print(\"\\nTraining model...\")\n    history = model.fit(\n        train_generator,\n        validation_data=val_generator,\n        epochs=50,\n        callbacks=[checkpoint_callback, early_stopping, reduce_lr]\n    )\n    \n    # Evaluate on test set\n    print(\"\\nEvaluating on test set...\")\n    test_loss, test_accuracy = model.evaluate(test_generator)\n    print(f\"\\nTest accuracy: {test_accuracy*100:.2f}%\")\n    \n    # Save training history\n    np.save(os.path.join(OUTPUT_PATH, 'training_history.npy'), history.history)\n    \n    return model, history\n\n# Run training\nif __name__ == \"__main__\":\n    print(\"Starting violence detection model training...\")\n    model, history = train_model()\n    print(\"\\nTraining completed. All results saved to:\", OUTPUT_PATH)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}