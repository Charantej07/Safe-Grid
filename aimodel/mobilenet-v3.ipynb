{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":397693,"sourceType":"datasetVersion","datasetId":176381},{"sourceId":4718786,"sourceType":"datasetVersion","datasetId":2730182}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import MobileNetV3Small\nfrom tensorflow.keras import layers, models\nimport cv2\nimport os\nimport numpy as np\nfrom tensorflow.keras.callbacks import EarlyStopping , ReduceLROnPlateau\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\nfrom tensorflow.keras.applications import MobileNetV3Small\nfrom tensorflow.keras import Input\nprint('DONE')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-12T09:50:12.288328Z","iopub.execute_input":"2025-04-12T09:50:12.288678Z","iopub.status.idle":"2025-04-12T09:50:24.212663Z","shell.execute_reply.started":"2025-04-12T09:50:12.288640Z","shell.execute_reply":"2025-04-12T09:50:24.211748Z"}},"outputs":[{"name":"stdout","text":"DONE\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import cv2\nimport os\nimport random\nfrom tqdm import tqdm\n\n# Define paths for validation folders\nval_violence_path = \"/kaggle/input/rwf2000/RWF-2000/val/Fight\"\nval_nonviolence_path = \"/kaggle/input/rwf2000/RWF-2000/val/NonFight\"\n\n# Output paths for extracted frames\noutput_paths = {\n    \"train_violence\": \"/kaggle/working/extracted_frames/train/violence\",\n    \"train_nonviolence\": \"/kaggle/working/extracted_frames/train/nonviolence\",\n    \"val_violence\": \"/kaggle/working/extracted_frames/val/violence\",\n    \"val_nonviolence\": \"/kaggle/working/extracted_frames/val/nonviolence\"\n}\n\n# Create output directories if they don't exist\nfor path in output_paths.values():\n    os.makedirs(path, exist_ok=True)\n\n# Function to extract frames at a rate of 3 frames per second\ndef extract_frames(video_path, output_folder, target_fps=3):\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        print(f\"Error opening video file: {video_path}\")\n        return\n    \n    original_fps = cap.get(cv2.CAP_PROP_FPS)\n    frame_interval = int(original_fps / target_fps)\n    frame_count, saved_frame_count = 0, 0\n    \n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        if frame_count % frame_interval == 0:\n            frame_filename = f\"{os.path.splitext(os.path.basename(video_path))[0]}_frame{saved_frame_count}.jpg\"\n            cv2.imwrite(os.path.join(output_folder, frame_filename), frame)\n            saved_frame_count += 1\n        frame_count += 1\n    cap.release()\n\n# Function to process videos, splitting 10% for validation if needed\ndef process_videos(video_paths, train_output, val_output, split_for_val=False, target_fps=3):\n    for folder in video_paths:\n        video_files = [f for f in os.listdir(folder) if f.lower().endswith(('.mp4', '.avi', '.mov', '.mkv'))]\n        \n        if split_for_val:\n            random.shuffle(video_files)\n            val_count = max(1, int(len(video_files) * 0.2))  \n            val_videos, train_videos = video_files[:val_count], video_files[val_count:]\n        else:\n            val_videos, train_videos = [], video_files  # All to training if no split\n        \n        # Extract frames from validation videos\n        for video_file in tqdm(val_videos, desc=f\"Extracting validation frames from {folder}\"):\n            extract_frames(os.path.join(folder, video_file), val_output, target_fps=target_fps)\n\n        # Extract frames from training videos\n        for video_file in tqdm(train_videos, desc=f\"Extracting training frames from {folder}\"):\n            extract_frames(os.path.join(folder, video_file), train_output, target_fps=target_fps)\n\n# Process violence and nonviolence folders with a 10% split for validation\nprocess_videos(\n    [\"/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence\"],\n    output_paths[\"train_violence\"],\n    output_paths[\"val_violence\"],\n    split_for_val=True\n)\nprocess_videos(\n    [\"/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence\"],\n    output_paths[\"train_nonviolence\"],\n    output_paths[\"val_nonviolence\"],\n    split_for_val=True\n)\n\n# Process all files from RWF-2000 train folders as training data (no validation split)\nprocess_videos(\n    [\"/kaggle/input/rwf2000/RWF-2000/train/Fight\"],\n    output_paths[\"train_violence\"],\n    output_paths[\"val_violence\"],\n    split_for_val=True\n)\nprocess_videos(\n    [\"/kaggle/input/rwf2000/RWF-2000/train/NonFight\"],\n    output_paths[\"train_nonviolence\"],\n    output_paths[\"val_nonviolence\"],\n    split_for_val=True\n)\n\n\nprint(\"Frame extraction completed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T09:50:24.214652Z","iopub.execute_input":"2025-04-12T09:50:24.215171Z","iopub.status.idle":"2025-04-12T10:02:56.546279Z","shell.execute_reply.started":"2025-04-12T09:50:24.215143Z","shell.execute_reply":"2025-04-12T10:02:56.545449Z"}},"outputs":[{"name":"stderr","text":"Extracting validation frames from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence: 100%|██████████| 200/200 [00:32<00:00,  6.14it/s]\nExtracting training frames from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence: 100%|██████████| 800/800 [02:32<00:00,  5.25it/s]\nExtracting validation frames from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence: 100%|██████████| 200/200 [00:21<00:00,  9.40it/s]\nExtracting training frames from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence:  74%|███████▍  | 593/800 [00:57<00:26,  7.81it/s][h264 @ 0x57d002aa7540] mb_type 104 in P slice too large at 98 31\n[h264 @ 0x57d002aa7540] error while decoding MB 98 31\nExtracting training frames from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence: 100%|██████████| 800/800 [01:15<00:00, 10.53it/s]\nExtracting validation frames from /kaggle/input/rwf2000/RWF-2000/train/Fight: 100%|██████████| 160/160 [00:55<00:00,  2.86it/s]\nExtracting training frames from /kaggle/input/rwf2000/RWF-2000/train/Fight: 100%|██████████| 640/640 [03:32<00:00,  3.01it/s]\nExtracting validation frames from /kaggle/input/rwf2000/RWF-2000/train/NonFight: 100%|██████████| 160/160 [00:38<00:00,  4.14it/s]\nExtracting training frames from /kaggle/input/rwf2000/RWF-2000/train/NonFight: 100%|██████████| 640/640 [02:41<00:00,  3.95it/s]","output_type":"stream"},{"name":"stdout","text":"Frame extraction completed.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\n\n# Specify the path to the main directory\nmain_dir = '/kaggle/working/extracted_frames'\n\n# Dictionary to store file counts\nfile_counts = {}\n\n# Walk through each subdirectory and count files\nfor root, dirs, files in os.walk(main_dir):\n    # Only count files, skip directories\n    file_counts[root] = len(files)\n\n# Print the file counts in each folder\nfor folder, count in file_counts.items():\n    print(f\"{folder}: {count} files\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T10:02:56.547319Z","iopub.execute_input":"2025-04-12T10:02:56.547578Z","iopub.status.idle":"2025-04-12T10:02:56.602269Z","shell.execute_reply.started":"2025-04-12T10:02:56.547551Z","shell.execute_reply":"2025-04-12T10:02:56.601493Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/extracted_frames: 0 files\n/kaggle/working/extracted_frames/train: 0 files\n/kaggle/working/extracted_frames/train/nonviolence: 22630 files\n/kaggle/working/extracted_frames/train/violence: 23781 files\n/kaggle/working/extracted_frames/val: 0 files\n/kaggle/working/extracted_frames/val/nonviolence: 6156 files\n/kaggle/working/extracted_frames/val/violence: 5546 files\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Paths to augmented frames for training and validation\ntrain_dir = '/kaggle/working/extracted_frames/train'\nval_dir = '/kaggle/working/extracted_frames/val'\n# Image size and batch size\nIMG_SIZE = (224, 224)\nBATCH_SIZE = 16\n\n# Data augmentation and generators\ntrain_datagen = ImageDataGenerator(\n    rescale=1.0/255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    brightness_range=[0.8, 1.2],\n    fill_mode='nearest'  \n)\n\n\nval_datagen = ImageDataGenerator(rescale=1.0/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    shuffle= True ,\n)\n\nval_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    shuffle=False,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T10:02:56.603159Z","iopub.execute_input":"2025-04-12T10:02:56.603417Z","iopub.status.idle":"2025-04-12T10:02:57.369489Z","shell.execute_reply.started":"2025-04-12T10:02:56.603391Z","shell.execute_reply":"2025-04-12T10:02:57.368824Z"}},"outputs":[{"name":"stdout","text":"Found 46411 images belonging to 2 classes.\nFound 11702 images belonging to 2 classes.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, Activation\nfrom tensorflow.keras.applications import MobileNetV3Small\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n\n# Define the MobileNetV3Small model\ninput_tensor = Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\nbase_model = MobileNetV3Small(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3), include_top=False, weights='imagenet')\n\nbase_model.trainable = True \n\n# Add custom layers on top of MobileNetV3Small\nx = base_model(input_tensor)\nx = GlobalAveragePooling2D()(x)\n\nx = Dense(1024)(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\n\nx = Dense(512)(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\n\noutput = Dense(1, activation='sigmoid')(x)\n\n# Build the model\nmodel = models.Model(inputs=input_tensor, outputs=output)\n\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T10:02:57.370564Z","iopub.execute_input":"2025-04-12T10:02:57.370922Z","iopub.status.idle":"2025-04-12T10:02:59.219296Z","shell.execute_reply.started":"2025-04-12T10:02:57.370885Z","shell.execute_reply":"2025-04-12T10:02:59.218591Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_small_224_1.0_float_no_top_v2.h5\n\u001b[1m4334752/4334752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MobilenetV3small (\u001b[38;5;33mFunctional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m576\u001b[0m)      │       \u001b[38;5;34m939,120\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │       \u001b[38;5;34m590,848\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_18 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_19 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m513\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MobilenetV3small (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">939,120</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,848</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,055,281\u001b[0m (7.84 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,055,281</span> (7.84 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,043,169\u001b[0m (7.79 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,043,169</span> (7.79 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m12,112\u001b[0m (47.31 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,112</span> (47.31 KB)\n</pre>\n"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint \n\nmodel.compile(optimizer=Adam(learning_rate=1e-4),  # Lower LR for fine-tuning\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Callbacks for adaptive learning rate and early stopping\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5)  \n#early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n        \ncheckpoint = ModelCheckpoint(\n    filepath='best_model.keras',  \n    monitor='val_loss',       \n    save_best_only=True,     \n    mode='min',              \n    verbose=1                \n)\n\nhistory = model.fit(\n    train_generator,\n    validation_data=val_generator,\n    epochs=30,\n    callbacks=[checkpoint,reduce_lr]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T10:02:59.221032Z","iopub.execute_input":"2025-04-12T10:02:59.221282Z","iopub.status.idle":"2025-04-12T15:17:49.146433Z","shell.execute_reply.started":"2025-04-12T10:02:59.221259Z","shell.execute_reply":"2025-04-12T15:17:49.145493Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1744452199.800784    8108 service.cc:145] XLA service 0x788084005530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1744452199.800840    8108 service.cc:153]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m   1/2901\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m53:37:27\u001b[0m 67s/step - accuracy: 0.5625 - loss: 0.6417","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1744452246.251009    8108 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.7251 - loss: 0.5139\nEpoch 1: val_loss improved from inf to 0.62080, saving model to best_model.keras\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m746s\u001b[0m 234ms/step - accuracy: 0.7251 - loss: 0.5139 - val_accuracy: 0.6466 - val_loss: 0.6208 - learning_rate: 1.0000e-04\nEpoch 2/30\n\u001b[1m2900/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.8777 - loss: 0.2764\nEpoch 2: val_loss did not improve from 0.62080\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m641s\u001b[0m 220ms/step - accuracy: 0.8777 - loss: 0.2764 - val_accuracy: 0.5635 - val_loss: 1.3486 - learning_rate: 1.0000e-04\nEpoch 3/30\n\u001b[1m2900/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.9161 - loss: 0.1991\nEpoch 3: val_loss did not improve from 0.62080\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m631s\u001b[0m 217ms/step - accuracy: 0.9161 - loss: 0.1991 - val_accuracy: 0.5355 - val_loss: 2.7459 - learning_rate: 1.0000e-04\nEpoch 4/30\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.9331 - loss: 0.1589\nEpoch 4: val_loss did not improve from 0.62080\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m625s\u001b[0m 215ms/step - accuracy: 0.9331 - loss: 0.1589 - val_accuracy: 0.5383 - val_loss: 2.7776 - learning_rate: 1.0000e-04\nEpoch 5/30\n\u001b[1m2899/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.9505 - loss: 0.1237\nEpoch 5: val_loss did not improve from 0.62080\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m614s\u001b[0m 211ms/step - accuracy: 0.9505 - loss: 0.1237 - val_accuracy: 0.6980 - val_loss: 0.8670 - learning_rate: 1.0000e-04\nEpoch 6/30\n\u001b[1m2900/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.9554 - loss: 0.1096\nEpoch 6: val_loss did not improve from 0.62080\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 210ms/step - accuracy: 0.9554 - loss: 0.1096 - val_accuracy: 0.6574 - val_loss: 1.0200 - learning_rate: 1.0000e-04\nEpoch 7/30\n\u001b[1m2900/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.9689 - loss: 0.0771\nEpoch 7: val_loss did not improve from 0.62080\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m613s\u001b[0m 211ms/step - accuracy: 0.9689 - loss: 0.0771 - val_accuracy: 0.8366 - val_loss: 0.8320 - learning_rate: 5.0000e-05\nEpoch 8/30\n\u001b[1m2900/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.9718 - loss: 0.0732\nEpoch 8: val_loss did not improve from 0.62080\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m613s\u001b[0m 211ms/step - accuracy: 0.9718 - loss: 0.0732 - val_accuracy: 0.8072 - val_loss: 0.8683 - learning_rate: 5.0000e-05\nEpoch 9/30\n\u001b[1m2900/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.9733 - loss: 0.0661\nEpoch 9: val_loss did not improve from 0.62080\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m614s\u001b[0m 211ms/step - accuracy: 0.9733 - loss: 0.0661 - val_accuracy: 0.8266 - val_loss: 0.6446 - learning_rate: 5.0000e-05\nEpoch 10/30\n\u001b[1m2900/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.9758 - loss: 0.0610\nEpoch 10: val_loss did not improve from 0.62080\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m614s\u001b[0m 211ms/step - accuracy: 0.9758 - loss: 0.0610 - val_accuracy: 0.6651 - val_loss: 1.1956 - learning_rate: 5.0000e-05\nEpoch 11/30\n\u001b[1m2899/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.9786 - loss: 0.0579\nEpoch 11: val_loss did not improve from 0.62080\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m618s\u001b[0m 212ms/step - accuracy: 0.9786 - loss: 0.0579 - val_accuracy: 0.7351 - val_loss: 1.1072 - learning_rate: 5.0000e-05\nEpoch 12/30\n\u001b[1m2899/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.9815 - loss: 0.0478\nEpoch 12: val_loss did not improve from 0.62080\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m612s\u001b[0m 211ms/step - accuracy: 0.9815 - loss: 0.0478 - val_accuracy: 0.7861 - val_loss: 0.7947 - learning_rate: 2.5000e-05\nEpoch 13/30\n\u001b[1m2900/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.9823 - loss: 0.0455\nEpoch 13: val_loss improved from 0.62080 to 0.51295, saving model to best_model.keras\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m613s\u001b[0m 211ms/step - accuracy: 0.9823 - loss: 0.0455 - val_accuracy: 0.8822 - val_loss: 0.5129 - learning_rate: 2.5000e-05\nEpoch 14/30\n\u001b[1m2899/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.9849 - loss: 0.0414\nEpoch 14: val_loss did not improve from 0.51295\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m609s\u001b[0m 209ms/step - accuracy: 0.9849 - loss: 0.0414 - val_accuracy: 0.7582 - val_loss: 1.0786 - learning_rate: 2.5000e-05\nEpoch 15/30\n\u001b[1m2900/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.9842 - loss: 0.0421\nEpoch 15: val_loss improved from 0.51295 to 0.50627, saving model to best_model.keras\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m612s\u001b[0m 210ms/step - accuracy: 0.9842 - loss: 0.0421 - val_accuracy: 0.9043 - val_loss: 0.5063 - learning_rate: 2.5000e-05\nEpoch 16/30\n\u001b[1m2899/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.9842 - loss: 0.0416\nEpoch 16: val_loss did not improve from 0.50627\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m629s\u001b[0m 216ms/step - accuracy: 0.9842 - loss: 0.0416 - val_accuracy: 0.8680 - val_loss: 0.6420 - learning_rate: 2.5000e-05\nEpoch 17/30\n\u001b[1m2899/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.9836 - loss: 0.0415\nEpoch 17: val_loss did not improve from 0.50627\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m625s\u001b[0m 215ms/step - accuracy: 0.9836 - loss: 0.0415 - val_accuracy: 0.8797 - val_loss: 0.5232 - learning_rate: 2.5000e-05\nEpoch 18/30\n\u001b[1m2899/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.9861 - loss: 0.0352\nEpoch 18: val_loss did not improve from 0.50627\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m626s\u001b[0m 215ms/step - accuracy: 0.9861 - loss: 0.0352 - val_accuracy: 0.7908 - val_loss: 0.8939 - learning_rate: 2.5000e-05\nEpoch 19/30\n\u001b[1m2900/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.9850 - loss: 0.0383\nEpoch 19: val_loss did not improve from 0.50627\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m623s\u001b[0m 214ms/step - accuracy: 0.9850 - loss: 0.0383 - val_accuracy: 0.7747 - val_loss: 0.8634 - learning_rate: 2.5000e-05\nEpoch 20/30\n\u001b[1m2899/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.9869 - loss: 0.0345\nEpoch 20: val_loss did not improve from 0.50627\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m619s\u001b[0m 213ms/step - accuracy: 0.9869 - loss: 0.0345 - val_accuracy: 0.8514 - val_loss: 0.8183 - learning_rate: 2.5000e-05\nEpoch 21/30\n\u001b[1m2900/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.9871 - loss: 0.0343\nEpoch 21: val_loss did not improve from 0.50627\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m622s\u001b[0m 214ms/step - accuracy: 0.9871 - loss: 0.0343 - val_accuracy: 0.8776 - val_loss: 0.6693 - learning_rate: 1.2500e-05\nEpoch 22/30\n\u001b[1m2900/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.9882 - loss: 0.0310\nEpoch 22: val_loss did not improve from 0.50627\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m625s\u001b[0m 215ms/step - accuracy: 0.9882 - loss: 0.0310 - val_accuracy: 0.8879 - val_loss: 0.5687 - learning_rate: 1.2500e-05\nEpoch 23/30\n\u001b[1m2900/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.9885 - loss: 0.0305\nEpoch 23: val_loss did not improve from 0.50627\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m639s\u001b[0m 220ms/step - accuracy: 0.9885 - loss: 0.0305 - val_accuracy: 0.8924 - val_loss: 0.6478 - learning_rate: 1.2500e-05\nEpoch 24/30\n\u001b[1m2900/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.9893 - loss: 0.0276\nEpoch 24: val_loss did not improve from 0.50627\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m656s\u001b[0m 226ms/step - accuracy: 0.9893 - loss: 0.0276 - val_accuracy: 0.8785 - val_loss: 0.6928 - learning_rate: 1.2500e-05\nEpoch 25/30\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.9883 - loss: 0.0303\nEpoch 25: val_loss did not improve from 0.50627\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m634s\u001b[0m 218ms/step - accuracy: 0.9883 - loss: 0.0303 - val_accuracy: 0.8836 - val_loss: 0.6077 - learning_rate: 1.2500e-05\nEpoch 26/30\n\u001b[1m2900/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.9905 - loss: 0.0262\nEpoch 26: val_loss did not improve from 0.50627\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m638s\u001b[0m 220ms/step - accuracy: 0.9905 - loss: 0.0262 - val_accuracy: 0.8998 - val_loss: 0.5882 - learning_rate: 1.0000e-05\nEpoch 27/30\n\u001b[1m2899/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.9888 - loss: 0.0289\nEpoch 27: val_loss did not improve from 0.50627\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m640s\u001b[0m 220ms/step - accuracy: 0.9888 - loss: 0.0289 - val_accuracy: 0.8847 - val_loss: 0.6408 - learning_rate: 1.0000e-05\nEpoch 28/30\n\u001b[1m2900/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.9893 - loss: 0.0264\nEpoch 28: val_loss did not improve from 0.50627\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m642s\u001b[0m 221ms/step - accuracy: 0.9893 - loss: 0.0264 - val_accuracy: 0.8856 - val_loss: 0.6715 - learning_rate: 1.0000e-05\nEpoch 29/30\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.9901 - loss: 0.0262\nEpoch 29: val_loss did not improve from 0.50627\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m635s\u001b[0m 218ms/step - accuracy: 0.9901 - loss: 0.0262 - val_accuracy: 0.8990 - val_loss: 0.6355 - learning_rate: 1.0000e-05\nEpoch 30/30\n\u001b[1m2900/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - accuracy: 0.9906 - loss: 0.0253\nEpoch 30: val_loss did not improve from 0.50627\n\u001b[1m2901/2901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m648s\u001b[0m 223ms/step - accuracy: 0.9906 - loss: 0.0253 - val_accuracy: 0.8961 - val_loss: 0.5932 - learning_rate: 1.0000e-05\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nimport numpy as np\n\n# Step 1: Evaluate the model using the validation generator\nloss, accuracy = model.evaluate(val_generator, verbose=1)\nprint(f\"Validation Loss: {loss}\")\nprint(f\"Validation Accuracy: {accuracy}\")\n\n# Step 2: Generate predictions\npredictions = model.predict(val_generator, verbose=1)          \npredicted_classes = (predictions > 0.5).astype(int).reshape(-1)  # Binary classification: Threshold at 0.5\n\n# Step 3: Get true labels from the generator\ntrue_classes = val_generator.classes  # Assuming this provides the true labels for each sample\nclass_labels = ['Class 0', 'Class 1']  # Replace with actual class names if available\n\n# Step 4: Generate classification report\nprint(\"\\nClassification Report:\\n\")\nprint(classification_report(true_classes, predicted_classes, target_names=class_labels))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T15:17:49.148012Z","iopub.execute_input":"2025-04-12T15:17:49.148651Z","iopub.status.idle":"2025-04-12T15:18:56.623081Z","shell.execute_reply.started":"2025-04-12T15:17:49.148611Z","shell.execute_reply":"2025-04-12T15:18:56.622146Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m732/732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 40ms/step - accuracy: 0.9114 - loss: 0.5142\nValidation Loss: 0.5931624174118042\nValidation Accuracy: 0.8960861563682556\n\u001b[1m732/732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 46ms/step\n\nClassification Report:\n\n              precision    recall  f1-score   support\n\n     Class 0       0.90      0.91      0.90      6156\n     Class 1       0.90      0.88      0.89      5546\n\n    accuracy                           0.90     11702\n   macro avg       0.90      0.90      0.90     11702\nweighted avg       0.90      0.90      0.90     11702\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Save the model in .keras format\nmodel.save(\"movileV3_89.keras\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T15:18:56.624379Z","iopub.execute_input":"2025-04-12T15:18:56.625349Z","iopub.status.idle":"2025-04-12T15:18:57.083206Z","shell.execute_reply.started":"2025-04-12T15:18:56.625304Z","shell.execute_reply":"2025-04-12T15:18:57.082271Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"pip install tf2onnx\nD","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T15:18:57.084367Z","iopub.execute_input":"2025-04-12T15:18:57.084641Z","iopub.status.idle":"2025-04-12T15:19:07.574432Z","shell.execute_reply.started":"2025-04-12T15:18:57.084615Z","shell.execute_reply":"2025-04-12T15:19:07.573287Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting tf2onnx\n  Downloading tf2onnx-1.16.1-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: numpy>=1.14.1 in /opt/conda/lib/python3.10/site-packages (from tf2onnx) (1.26.4)\nRequirement already satisfied: onnx>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from tf2onnx) (1.17.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from tf2onnx) (2.32.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from tf2onnx) (1.16.0)\nRequirement already satisfied: flatbuffers>=1.12 in /opt/conda/lib/python3.10/site-packages (from tf2onnx) (24.3.25)\nRequirement already satisfied: protobuf~=3.20 in /opt/conda/lib/python3.10/site-packages (from tf2onnx) (3.20.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->tf2onnx) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->tf2onnx) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->tf2onnx) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->tf2onnx) (2024.8.30)\nDownloading tf2onnx-1.16.1-py3-none-any.whl (455 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.8/455.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tf2onnx\nSuccessfully installed tf2onnx-1.16.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import tf2onnx\nimport tensorflow as tf\n\n# Assuming 'model' is your trained Keras or TensorFlow model\n\n# Define the path to save the ONNX model\nonnx_model_path = \"movileV3_89.onnx\"\n\n# Convert the model to ONNX format\nspec = (tf.TensorSpec(model.inputs[0].shape, tf.float32, name=\"input\"),)\nonnx_model, _ = tf2onnx.convert.from_keras(model, input_signature=spec)\n\n# Save the ONNX model to the specified path\nwith open(onnx_model_path, \"wb\") as f:\n    f.write(onnx_model.SerializeToString())\n\nprint(f\"Model saved in ONNX format at {onnx_model_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T15:19:07.576147Z","iopub.execute_input":"2025-04-12T15:19:07.576494Z","iopub.status.idle":"2025-04-12T15:19:12.379929Z","shell.execute_reply.started":"2025-04-12T15:19:07.576463Z","shell.execute_reply":"2025-04-12T15:19:12.378977Z"}},"outputs":[{"name":"stdout","text":"Model saved in ONNX format at movileV3_89.onnx\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from tensorflow import keras\nmodel = keras.models.load_model(\"/kaggle/working/best_model.keras\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T15:21:08.531904Z","iopub.execute_input":"2025-04-12T15:21:08.532322Z","iopub.status.idle":"2025-04-12T15:21:10.219070Z","shell.execute_reply.started":"2025-04-12T15:21:08.532288Z","shell.execute_reply":"2025-04-12T15:21:10.218074Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\ndef extract_frames_for_prediction(video_path, target_size=(224, 224), frame_rate=3, max_frames=30):\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    interval = int(fps / frame_rate)\n    \n    count, extracted = 0, 0\n    while cap.isOpened() and extracted < max_frames:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        if count % interval == 0:\n            frame = cv2.resize(frame, target_size)\n            frame = frame.astype(\"float32\") / 255.0\n            frames.append(frame)\n            extracted += 1\n        count += 1\n    cap.release()\n    return np.array(frames)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T15:21:37.509188Z","iopub.execute_input":"2025-04-12T15:21:37.509869Z","iopub.status.idle":"2025-04-12T15:21:37.516410Z","shell.execute_reply.started":"2025-04-12T15:21:37.509832Z","shell.execute_reply":"2025-04-12T15:21:37.515463Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"video_path = \"/kaggle/input/rwf2000/RWF-2000/val/Fight/1MVS2QPWbHc_0.avi\"\nframes = extract_frames_for_prediction(video_path)\n\n# Average prediction across all frames\npredictions = model.predict(frames)\navg_pred = np.mean(predictions)\n\nlabel = \"Violent\" if avg_pred > 0.5 else \"Non-Violent\"\nprint(f\"Prediction: {label} ({avg_pred:.4f})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T15:22:47.593082Z","iopub.execute_input":"2025-04-12T15:22:47.594203Z","iopub.status.idle":"2025-04-12T15:22:48.229511Z","shell.execute_reply.started":"2025-04-12T15:22:47.594165Z","shell.execute_reply":"2025-04-12T15:22:48.228820Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\nPrediction: Violent (0.9049)\n","output_type":"stream"}],"execution_count":14}]}